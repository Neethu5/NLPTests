{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 4), match='Mary'> ['Mary', 'had', 'a', 'little', 'lamb']\n"
     ]
    }
   ],
   "source": [
    "# find words\n",
    "phrase = \"Mary had a little lamb.\"\n",
    "pattern = \"\\w+\"\n",
    "m = re.match(pattern, phrase)\n",
    "m2 = re.findall(pattern, phrase)\n",
    "print(m, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None <_sre.SRE_Match object; span=(16, 17), match='2'>\n"
     ]
    }
   ],
   "source": [
    "# find digits\n",
    "phrase = \"I called you at 2 o'clock\"\n",
    "pattern = \"\\d\"\n",
    "# match requires the beginning of the string to be right\n",
    "m = re.match(pattern, phrase)\n",
    "# search produces any results in the string\n",
    "s = re.search(pattern, phrase)\n",
    "print(m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'called', 'you', 'at', '2', \"o'clock\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find spaces\n",
    "phrase = \"I called you at 2 o'clock\"\n",
    "pattern = \"\\s+\"\n",
    "re.split(pattern, phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 39), match='match lowercase spaces and nums like 12'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"match lowercase spaces and nums like 12, but no commas\"\n",
    "pattern = '[a-z0-9 ]+'\n",
    "re.match(pattern, phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', \"n't\", 'like', 'Sam', \"'s\", 'shoes', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"I don't like Sam's shoes.\"\n",
    "word_tokenize(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#',\n",
       " '1',\n",
       " ':',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " \"'s\",\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "word_tokenize(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a regex for tokenization. Just getting sentence ponctuation but no commas or colons and keeping #1 intact\n",
    "pattern = r\"(\\w+|#\\d|\\?|!)\"\n",
    "regexp_tokenize(phrase, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER', 'Found', 'In', 'Mercea', 'The']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "regexp_tokenize(phrase, capital_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'my',\n",
       " 'way',\n",
       " 'to',\n",
       " 'the',\n",
       " '@WomenInGaming',\n",
       " 'rally',\n",
       " 'at',\n",
       " '@Official_GDC',\n",
       " '.',\n",
       " 'My',\n",
       " 'absolute',\n",
       " 'fav',\n",
       " 'day',\n",
       " 'for',\n",
       " '#WiG',\n",
       " 'every',\n",
       " 'year',\n",
       " '.',\n",
       " 'Many',\n",
       " 'gaming',\n",
       " 'industry',\n",
       " 'folks',\n",
       " 'in',\n",
       " 'this',\n",
       " 'flight',\n",
       " ',',\n",
       " 'lovely',\n",
       " 'running',\n",
       " 'into',\n",
       " '@SweetMicga',\n",
       " 'ðŸ’š']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"On my way to the @WomenInGaming rally at @Official_GDC. My absolute fav day for #WiG every year. Many gaming industry folks in this flight, lovely running into @SweetMicga ðŸ’š\"\n",
    "t = TweetTokenizer()\n",
    "t.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "words = word_tokenize(\"This is a pretty cool tool!\")\n",
    "word_lengths = [len(w) for w in words]\n",
    "\n",
    "plt.hist(word_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading Monty Python's Holy Grail script\n",
    "holy_grail = nltk.corpus.webtext.raw('grail.txt')\n",
    "holy_grail[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] ',\n",
       " 'KING ARTHUR: Whoa there!  [clop clop clop] ',\n",
       " 'SOLDIER #1: Halt!  Who goes there?',\n",
       " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'SOLDIER #1: Pull the other one!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting into lines\n",
    "lines = holy_grail.split('\\n')\n",
    "lines[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] ',\n",
       " ' Whoa there!  [clop clop clop] ',\n",
       " ' Halt!  Who goes there?',\n",
       " ' It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " ' Pull the other one!']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "lines[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Halt', 'Who', 'goes', 'there']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "tokenized_lines[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '.': 3,\n",
       "         'likes': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(word_tokenize(\n",
    "        \"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"))\n",
    "\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = '\\'\\'\\'Debugging\\'\\'\\' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \\n\\nNumerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statistical Process Control, and special design tactics to improve detection while simplifying changes.\\n\\nOrigin\\nA computer log entry from the Mark&nbsp;II, with a moth taped to the page\\n\\nThe terms \"bug\" and \"debugging\" are popularly attributed to Admiral Grace Hopper in the 1940s.[http://foldoc.org/Grace+Hopper Grace Hopper]  from FOLDOC While she was working on a Harvard Mark II|Mark II Computer at Harvard University, her associates discovered a moth stuck in a relay and thereby impeding operation, whereupon she remarked that they were \"debugging\" the system. However the term \"bug\" in the meaning of technical error dates back at least to 1878 and Thomas Edison (see software bug for a full discussion), and \"debugging\" seems to have been used as a term in aeronautics before entering the world of computers. Indeed, in an interview Grace Hopper remarked that she was not coining the term{{Citation needed|date=July 2015}}. The moth fit the already existing terminology, so it was saved.  A letter from J. Robert Oppenheimer (director of the WWII atomic bomb \"Manhattan\" project at Los Alamos, NM) used the term in a letter to Dr. Ernest Lawrence at UC Berkeley, dated October 27, 1944,http://bancroft.berkeley.edu/Exhibits/physics/images/bigscience25.jpg regarding the recruitment of additional technical staff.\\n\\nThe Oxford English Dictionary entry for \"debug\" quotes the term \"debugging\" used in reference to airplane engine testing in a 1945 article in the Journal of the Royal Aeronautical Society. An article in \"Airforce\" (June 1945 p.&nbsp;50) also refers to debugging, this time of aircraft cameras.  Hopper\\'s computer bug|bug was found on September 9, 1947. The term was not adopted by computer programmers until the early 1950s.\\nThe seminal article by GillS. Gill, [http://www.jstor.org/stable/98663 The Diagnosis of Mistakes in Programmes on the EDSAC], Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, Vol. 206, No. 1087 (May 22, 1951), pp. 538-554 in 1951 is the earliest in-depth discussion of programming errors, but it does not use the term \"bug\" or \"debugging\".\\nIn the Association for Computing Machinery|ACM\\'s digital library, the term \"debugging\" is first used in three papers from 1952 ACM National Meetings.Robert V. D. Campbell, [http://portal.acm.org/citation.cfm?id=609784.609786 Evolution of automatic computation], Proceedings of the 1952 ACM national meeting (Pittsburgh), p 29-32, 1952.Alex Orden, [http://portal.acm.org/citation.cfm?id=609784.609793 Solution of systems of linear inequalities on a digital computer], Proceedings of the 1952 ACM national meeting (Pittsburgh), p. 91-95, 1952.Howard B. Demuth, John B. Jackson, Edmund Klein, N. Metropolis, Walter Orvedahl, James H. Richardson, [http://portal.acm.org/citation.cfm?id=800259.808982 MANIAC], Proceedings of the 1952 ACM national meeting (Toronto), p. 13-16 Two of the three use the term in quotation marks.\\nBy 1963 \"debugging\" was a common enough term to be mentioned in passing without explanation on page 1 of the Compatible Time-Sharing System|CTSS manual.[http://www.bitsavers.org/pdf/mit/ctss/CTSS_ProgrammersGuide.pdf The Compatible Time-Sharing System], M.I.T. Press, 1963\\n\\nKidwell\\'s article \\'\\'Stalking the Elusive Computer Bug\\'\\'Peggy Aldrich Kidwell, [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?tp=&arnumber=728224&isnumber=15706 Stalking the Elusive Computer Bug], IEEE Annals of the History of Computing, 1998. discusses the etymology of \"bug\" and \"debug\" in greater detail.\\n\\nScope\\nAs software and electronic systems have become generally more complex, the various common debugging techniques have expanded with more methods to detect anomalies, assess impact, and schedule software patches or full updates to a system. The words \"anomaly\" and \"discrepancy\" can be used, as being more neutral terms, to avoid the words \"error\" and \"defect\" or \"bug\" where there might be an implication that all so-called \\'\\'errors\\'\\', \\'\\'defects\\'\\' or \\'\\'bugs\\'\\' must be fixed (at all costs). Instead, an impact assessment can be made to determine if changes to remove an \\'\\'anomaly\\'\\' (or \\'\\'discrepancy\\'\\') would be cost-effective for the system, or perhaps a scheduled new release might render the change(s) unnecessary. Not all issues are life-critical or mission-critical in a system. Also, it is important to avoid the situation where a change might be more upsetting to users, long-term, than living with the known problem(s) (where the \"cure would be worse than the disease\"). Basing decisions of the acceptability of some anomalies can avoid a culture of a \"zero-defects\" mandate, where people might be tempted to deny the existence of problems so that the result would appear as zero \\'\\'defects\\'\\'. Considering the collateral issues, such as the cost-versus-benefit impact assessment, then broader debugging techniques will expand to determine the frequency of anomalies (how often the same \"bugs\" occur) to help assess their impact to the overall system.\\n\\nTools\\nDebugging on video game consoles is usually done with special hardware such as this Xbox (console)|Xbox debug unit intended for developers.\\n\\nDebugging ranges in complexity from fixing simple errors to performing lengthy and tiresome tasks of data collection, analysis, and scheduling updates.  The debugging skill of the programmer can be a major factor in the ability to debug a problem, but the difficulty of software debugging varies greatly with the complexity of the system, and also depends, to some extent, on the programming language(s) used and the available tools, such as \\'\\'debuggers\\'\\'. Debuggers are software tools which enable the programmer to monitor the execution (computers)|execution of a program, stop it, restart it, set breakpoints, and change values in memory. The term \\'\\'debugger\\'\\' can also refer to the person who is doing the debugging.\\n\\nGenerally, high-level programming languages, such as Java (programming language)|Java, make debugging easier, because they have features such as exception handling that make real sources of erratic behaviour easier to spot. In programming languages such as C (programming language)|C or assembly language|assembly, bugs may cause silent problems such as memory corruption, and it is often difficult to see where the initial problem happened. In those cases, memory debugging|memory debugger tools may be needed.\\n\\nIn certain situations, general purpose software tools that are language specific in nature can be very useful.  These take the form of \\'\\'List of tools for static code analysis|static code analysis tools\\'\\'.  These tools look for a very specific set of known problems, some common and some rare, within the source code.  All such issues detected by these tools would rarely be picked up by a compiler or interpreter, thus they are not syntax checkers, but more semantic checkers.  Some tools claim to be able to detect 300+ unique problems. Both commercial and free tools exist in various languages.  These tools can be extremely useful when checking very large source trees, where it is impractical to do code walkthroughs.  A typical example of a problem detected would be a variable dereference that occurs \\'\\'before\\'\\' the variable is assigned a value.  Another example would be to perform strong type checking when the language does not require such.  Thus, they are better at locating likely errors, versus actual errors.  As a result, these tools have a reputation of false positives.  The old Unix \\'\\'Lint programming tool|lint\\'\\' program is an early example.\\n\\nFor debugging electronic hardware (e.g., computer hardware) as well as low-level software (e.g., BIOSes, device drivers) and firmware, instruments such as oscilloscopes, logic analyzers or in-circuit emulator|in-circuit emulators (ICEs) are often used, alone or in combination.  An ICE may perform many of the typical software debugger\\'s tasks on low-level software and firmware.\\n\\nDebugging process \\nNormally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with Parallel computing|parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.\\n\\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, a bug in a compiler can make it Crash (computing)|crash when parsing some large source file. However, after simplification of the test case, only few lines from the original source file can be sufficient to reproduce the same crash. Such simplification can be made manually, using a Divide and conquer algorithm|divide-and-conquer approach. The programmer will try to remove some parts of original test case and check if the problem still exists. When debugging the problem in a Graphical user interface|GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear.\\n\\nAfter the test case is sufficiently simplified, a programmer can use a debugger tool to examine program states (values of variables, plus the call stack) and track down the origin of the problem(s). Alternatively, Tracing (software)|tracing can be used. In simple cases, tracing is just a few print statements, which output the values of variables at certain points of program execution.{{citation needed|date=February 2016}}\\n\\n Techniques \\n \\'\\'Interactive debugging\\'\\'\\n \\'\\'{{visible anchor|Print debugging}}\\'\\' (or tracing) is the act of watching (live or recorded) trace statements, or print statements, that indicate the flow of execution of a process. This is sometimes called \\'\\'{{visible anchor|printf debugging}}\\'\\', due to the use of the printf function in C. This kind of debugging was turned on by the command TRON in the original versions of the novice-oriented BASIC programming language. TRON stood for, \"Trace On.\" TRON caused the line numbers of each BASIC command line to print as the program ran.\\n \\'\\'Remote debugging\\'\\' is the process of debugging a program running on a system different from the debugger. To start remote debugging, a debugger connects to a remote system over a network. The debugger can then control the execution of the program on the remote system and retrieve information about its state.\\n \\'\\'Post-mortem debugging\\'\\' is debugging of the program after it has already Crash (computing)|crashed. Related techniques often include various tracing techniques (for example,[http://www.drdobbs.com/tools/185300443 Postmortem Debugging, Stephen Wormuller, Dr. Dobbs Journal, 2006]) and/or analysis of memory dump (or core dump) of the crashed process. The dump of the process could be obtained automatically by the system (for example, when process has terminated due to an unhandled exception), or by a programmer-inserted instruction, or manually by the interactive user.\\n \\'\\'\"Wolf fence\" algorithm:\\'\\' Edward Gauss described this simple but very useful and now famous algorithm in a 1982 article for communications of the ACM as follows: \"There\\'s one wolf in Alaska; how do you find it? First build a fence down the middle of the state, wait for the wolf to howl, determine which side of the fence it is on. Repeat process on that side only, until you get to the point where you can see the wolf.\"<ref name=\"communications of the ACM\">{{cite journal | title=\"Pracniques: The \"Wolf Fence\" Algorithm for Debugging\", | author=E. J. Gauss | year=1982}} This is implemented e.g. in the Git (software)|Git version control system as the command \\'\\'git bisect\\'\\', which uses the above algorithm to determine which Commit (data management)|commit introduced a particular bug.\\n \\'\\'Delta Debugging\\'\\'{{snd}} a technique of automating test case simplification.Andreas Zeller: <cite>Why Programs Fail: A Guide to Systematic Debugging</cite>, Morgan Kaufmann, 2005. ISBN 1-55860-866-4{{rp|p.123}}<!-- for redirect from \\'Saff Squeeze\\' -->\\n \\'\\'Saff Squeeze\\'\\'{{snd}} a technique of isolating failure within the test using progressive inlining of parts of the failing test.[http://www.threeriversinstitute.org/HitEmHighHitEmLow.html Kent Beck, Hit \\'em High, Hit \\'em Low: Regression Testing and the Saff Squeeze]\\n\\nDebugging for embedded systems\\nIn contrast to the general purpose computer software design environment, a primary characteristic of embedded environments is the sheer number of different platforms available to the developers (CPU architectures, vendors, operating systems and their variants). Embedded systems are, by definition, not general-purpose designs: they are typically developed for a single task (or small range of tasks), and the platform is chosen specifically to optimize that application. Not only does this fact make life tough for embedded system developers, it also makes debugging and testing of these systems harder as well, since different debugging tools are needed in different platforms.\\n\\nto identify and fix bugs in the system (e.g. logical or synchronization problems in the code, or a design error in the hardware);\\nto collect information about the operating states of the system that may then be used to analyze the system: to find ways to boost its performance or to optimize other important characteristics (e.g. energy consumption, reliability, real-time response etc.).\\n\\nAnti-debugging\\nAnti-debugging is \"the implementation of one or more techniques within computer code that hinders attempts at reverse engineering or debugging a target process\".<ref name=\"veracode-antidebugging\">{{cite web |url=http://www.veracode.com/blog/2008/12/anti-debugging-series-part-i/ |title=Anti-Debugging Series - Part I |last=Shields |first=Tyler |date=2008-12-02 |work=Veracode |accessdate=2009-03-17}} It is actively used by recognized publishers in copy protection|copy-protection schemas, but is also used by malware to complicate its detection and elimination.<ref name=\"soft-prot\">[http://people.seas.harvard.edu/~mgagnon/software_protection_through_anti_debugging.pdf Software Protection through Anti-Debugging Michael N Gagnon, Stephen Taylor, Anup Ghosh] Techniques used in anti-debugging include:\\nAPI-based: check for the existence of a debugger using system information\\nException-based: check to see if exceptions are interfered with\\nProcess and thread blocks: check whether process and thread blocks have been manipulated\\nModified code: check for code modifications made by a debugger handling software breakpoints\\nHardware- and register-based: check for hardware breakpoints and CPU registers\\nTiming and latency: check the time taken for the execution of instructions\\nDetecting and penalizing debugger<ref name=\"soft-prot\" /><!-- reference does not exist -->\\n\\nAn early example of anti-debugging existed in early versions of Microsoft Word which, if a debugger was detected, produced a message that said: \"The tree of evil bears bitter fruit. Now trashing program disk.\", after which it caused the floppy disk drive to emit alarming noises with the intent of scaring the user away from attempting it again.<ref name=\"SecurityEngineeringRA\">{{cite book | url=http://www.cl.cam.ac.uk/~rja14/book.html | author=Ross J. Anderson | title=Security Engineering | isbn = 0-471-38922-6 | page=684 }}<ref name=\"toastytech\">{{cite web | url=http://toastytech.com/guis/word1153.html | title=Microsoft Word for DOS 1.15}}\\n'\n",
    "article = article.replace(\"\\'\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 150), ('.', 89), ('of', 81), ('to', 63), ('a', 60), ('in', 44), ('and', 41), (\"''\", 41), ('debugging', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very clean , we probably want to get rid of punctuation for this as we're trying to do topic analysis.\n",
    "\n",
    "### Preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"\n",
    "\n",
    "# removing punctuation\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'box', 'cat', 'likes', 'box', 'box', 'cat']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stopwords\n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('debugging', 40),\n",
       " ('system', 19),\n",
       " ('software', 16),\n",
       " ('tools', 14),\n",
       " ('process', 12),\n",
       " ('computer', 12),\n",
       " ('used', 12),\n",
       " ('http', 11),\n",
       " ('term', 11),\n",
       " ('bug', 10)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try the article again\n",
    "tokens = word_tokenize(article.lower())\n",
    "# punctuation\n",
    "tokens = [w for w in tokens if w.isalpha()]\n",
    "# stop words\n",
    "tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "# bag of words\n",
    "Counter(tokens).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally we can perform Lemmatization\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bug'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "wordnet_lemmatizer.lemmatize(\"bugs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('debugging', 40),\n",
       " ('system', 25),\n",
       " ('software', 16),\n",
       " ('bug', 16),\n",
       " ('problem', 15),\n",
       " ('tool', 15),\n",
       " ('computer', 14),\n",
       " ('process', 13),\n",
       " ('term', 13),\n",
       " ('debugger', 13)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "bow = Counter(lemmatized)\n",
    "bow.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice bug goes higher in the list as we're accounting for bug and bugs and possibly other variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using gensim for Topic identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "               'I really liked the movie!',\n",
    "               'Awesome action scenes, but boring characters.',\n",
    "               'The movie was awful! I hate alien films.',\n",
    "               'Space is cool! I liked the movie.',\n",
    "               'More space films, please!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['movie', 'spaceship', 'aliens'],\n",
       " ['really', 'liked', 'movie'],\n",
       " ['awesome', 'action', 'scenes', 'boring', 'characters'],\n",
       " ['movie', 'awful', 'hate', 'alien', 'films'],\n",
       " ['space', 'cool', 'liked', 'movie'],\n",
       " ['space', 'films', 'please']]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess\n",
    "d_tokens = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "\n",
    "proc_tokens = []\n",
    "for d in d_tokens:\n",
    "    # punctuation\n",
    "    tokens = [w for w in d if w.isalpha()]\n",
    "    # stop words\n",
    "    tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
    "    # bag of words\n",
    "    proc_tokens.append(tokens)\n",
    "    \n",
    "proc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = Dictionary(proc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [my_dict.doc2bow(doc) for doc in proc_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (3, 1), (4, 1)],\n",
       " [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1)],\n",
       " [(1, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(1, 1), (3, 1), (14, 1), (15, 1)],\n",
       " [(12, 1), (15, 1), (16, 1)]]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each document is a list of tuples with tokens and frequencies\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aliens': 0,\n",
       " 'movie': 1,\n",
       " 'spaceship': 2,\n",
       " 'liked': 3,\n",
       " 'really': 4,\n",
       " 'action': 5,\n",
       " 'awesome': 6,\n",
       " 'boring': 7,\n",
       " 'characters': 8,\n",
       " 'scenes': 9,\n",
       " 'alien': 10,\n",
       " 'awful': 11,\n",
       " 'films': 12,\n",
       " 'hate': 13,\n",
       " 'cool': 14,\n",
       " 'space': 15,\n",
       " 'please': 16}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see the dictionary and how it is made up\n",
    "my_dict.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict.token2id.get(\"movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie 4\n",
      "liked 2\n",
      "films 2\n",
      "space 2\n",
      "aliens 1\n"
     ]
    }
   ],
   "source": [
    "# get most frequent words across all documents\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing TF-IDF\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.12223812389456987),\n",
       " (10, 0.5401730299573403),\n",
       " (11, 0.5401730299573403),\n",
       " (12, 0.33120557692595515),\n",
       " (13, 0.5401730299573403)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = corpus[3]\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "tfidf_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alien 0.5401730299573403\n",
      "awful 0.5401730299573403\n",
      "hate 0.5401730299573403\n",
      "films 0.33120557692595515\n",
      "movie 0.12223812389456987\n"
     ]
    }
   ],
   "source": [
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'New',\n",
       " 'York',\n",
       " ',',\n",
       " 'I',\n",
       " 'like',\n",
       " 'to',\n",
       " 'ride',\n",
       " 'the',\n",
       " 'Metro',\n",
       " 'to',\n",
       " 'visit',\n",
       " 'MOMA',\n",
       " 'and',\n",
       " 'some',\n",
       " 'restaurants',\n",
       " 'rated',\n",
       " 'well',\n",
       " 'by',\n",
       " 'Rith',\n",
       " 'Reichl',\n",
       " '.']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '''In New York, I like to ride the Metro to visit MOMA and some restaurants rated well by Rith Reichl.'''\n",
    "\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('like', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('ride', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Metro', 'NNP'),\n",
       " ('to', 'TO'),\n",
       " ('visit', 'VB'),\n",
       " ('MOMA', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('some', 'DT'),\n",
       " ('restaurants', 'NNS'),\n",
       " ('rated', 'VBN'),\n",
       " ('well', 'RB'),\n",
       " ('by', 'IN'),\n",
       " ('Rith', 'NNP'),\n",
       " ('Reichl', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parts of speech tagging\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  ride/VB\n",
      "  the/DT\n",
      "  (ORGANIZATION Metro/NNP)\n",
      "  to/TO\n",
      "  visit/VB\n",
      "  (ORGANIZATION MOMA/NNP)\n",
      "  and/CC\n",
      "  some/DT\n",
      "  restaurants/NNS\n",
      "  rated/VBN\n",
      "  well/RB\n",
      "  by/IN\n",
      "  (PERSON Rith/NNP Reichl/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity chunks\n",
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see a more complex example\n",
    "article = '\\ufeffThe taxi-hailing company Uber brings into very sharp focus the question of whether corporations can be said to have a moral character. If any human being were to behave with the single-minded and ruthless greed of the company, we would consider them sociopathic. Uber wanted to know as much as possible about the people who use its service, and those who donâ€™t. It has an arrangement with unroll.me, a company which offered a free service for unsubscribing from junk mail, to buy the contacts unroll.me customers had had with rival taxi companies. Even if their email was notionally anonymised, this use of it was not something the users had bargained for. Beyond that, it keeps track of the phones that have been used to summon its services even after the original owner has sold them, attempting this with Appleâ€™s phones even thought it is forbidden by the company.\\r\\n\\r\\n\\r\\nUber has also tweaked its software so that regulatory agencies that the company regarded as hostile would, when they tried to hire a driver, be given false reports about the location of its cars. Uber management booked and then cancelled rides with a rival taxi-hailing company which took their vehicles out of circulation. Uber deny this was the intention. The punishment for this behaviour was negligible. Uber promised not to use this â€œgreyballâ€ software against law enforcement â€“ one wonders what would happen to someone carrying a knife who promised never to stab a policeman with it. Travis Kalanick of Uber got a personal dressing down from Tim Cook, who runs Apple, but the company did not prohibit the use of the app. Too much money was at stake for that.\\r\\n\\r\\n\\r\\nMillions of people around the world value the cheapness and convenience of Uberâ€™s rides too much to care about the lack of driversâ€™ rights or pay. Many of the users themselves are not much richer than the drivers. The â€œsharing economyâ€ encourages the insecure and exploited to exploit others equally insecure to the profit of a tiny clique of billionaires. Silicon Valleyâ€™s culture seems hostile to humane and democratic values. The outgoing CEO of Yahoo, Marissa Mayer, who is widely judged to have been a failure, is likely to get a $186m payout. This may not be a cause for panic, any more than the previous hero worship should have been a cause for euphoria. Yet thereâ€™s an urgent political task to tame these companies, to ensure they are punished when they break the law, that they pay their taxes fairly and that they behave responsibly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Uber/NNP)\n",
      "(NE Beyond/NN)\n",
      "(NE Apple/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Uber/NNP)\n",
      "(NE Travis/NNP Kalanick/NNP)\n",
      "(NE Tim/NNP Cook/NNP)\n",
      "(NE Apple/NNP)\n",
      "(NE Silicon/NNP Valley/NNP)\n",
      "(NE CEO/NNP)\n",
      "(NE Yahoo/NNP)\n",
      "(NE Marissa/NNP Mayer/NNP)\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(article)\n",
    "\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "         if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA15JREFUeJzt1DEBACAMwDDAv+dxIIIeiYJe3TOzAPjv/A4A4DFkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBogwZIAIQwaIMGSACEMGiDBkgAhDBoi44mIE2eKYV2AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spacy for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.2, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.pipes.EntityRecognizer at 0x242a681a168>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"\"\"Berlin is the capital of Germany; and the residence of Chancellor Angela Merkel.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Berlin, Germany, Angela Merkel)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin GPE\n",
      "Germany GPE\n",
      "Angela Merkel PERSON\n"
     ]
    }
   ],
   "source": [
    "for e in doc.ents:\n",
    "    print(e.text, e.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE Uber\n",
      "PERSON Uber\n",
      "ORG Apple\n",
      "PERSON Uber\n",
      "PERSON Uber\n",
      "PERSON Uber\n",
      "PERSON Uber\n",
      "GPE Travis Kalanick\n",
      "GPE Uber\n",
      "PERSON Tim Cook\n",
      "ORG Apple\n",
      "CARDINAL Millions\n",
      "ORG Uberâ€™s\n",
      "LOC Silicon Valleyâ€™s\n",
      "ORG Yahoo\n",
      "PERSON Marissa Mayer\n",
      "MONEY 186\n"
     ]
    }
   ],
   "source": [
    "# let's try our article about Uber\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for e in doc.ents:\n",
    "    print(e.label_, e.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-learn for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x16 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(my_documents)\n",
    "count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [doc for doc in count_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie': 11,\n",
       " 'spaceship': 15,\n",
       " 'aliens': 2,\n",
       " 'really': 12,\n",
       " 'liked': 10,\n",
       " 'awesome': 3,\n",
       " 'action': 0,\n",
       " 'scenes': 13,\n",
       " 'boring': 5,\n",
       " 'characters': 6,\n",
       " 'awful': 4,\n",
       " 'hate': 9,\n",
       " 'alien': 1,\n",
       " 'films': 8,\n",
       " 'space': 14,\n",
       " 'cool': 7}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['action',\n",
       " 'alien',\n",
       " 'aliens',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'boring',\n",
       " 'characters',\n",
       " 'cool',\n",
       " 'films',\n",
       " 'hate',\n",
       " 'liked',\n",
       " 'movie',\n",
       " 'really',\n",
       " 'scenes',\n",
       " 'space',\n",
       " 'spaceship']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.65205671, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.3868386 , 0.        , 0.        , 0.        ,\n",
       "         0.65205671]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(my_documents)\n",
    "\n",
    "corpus = [doc for doc in tfidf_train]\n",
    "corpus[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>alien</th>\n",
       "      <th>aliens</th>\n",
       "      <th>awesome</th>\n",
       "      <th>awful</th>\n",
       "      <th>boring</th>\n",
       "      <th>characters</th>\n",
       "      <th>cool</th>\n",
       "      <th>films</th>\n",
       "      <th>hate</th>\n",
       "      <th>liked</th>\n",
       "      <th>movie</th>\n",
       "      <th>really</th>\n",
       "      <th>scenes</th>\n",
       "      <th>space</th>\n",
       "      <th>spaceship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action  alien  aliens  awesome  awful  boring  characters  cool  films  \\\n",
       "0       0      0       1        0      0       0           0     0      0   \n",
       "1       0      0       0        0      0       0           0     0      0   \n",
       "2       1      0       0        1      0       1           1     0      0   \n",
       "3       0      1       0        0      1       0           0     0      1   \n",
       "4       0      0       0        0      0       0           0     1      0   \n",
       "\n",
       "   hate  liked  movie  really  scenes  space  spaceship  \n",
       "0     0      0      1       0       0      0          1  \n",
       "1     0      1      1       1       0      0          0  \n",
       "2     0      0      0       0       1      0          0  \n",
       "3     1      0      1       0       0      0          0  \n",
       "4     0      1      1       0       0      1          0  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>alien</th>\n",
       "      <th>aliens</th>\n",
       "      <th>awesome</th>\n",
       "      <th>awful</th>\n",
       "      <th>boring</th>\n",
       "      <th>characters</th>\n",
       "      <th>cool</th>\n",
       "      <th>films</th>\n",
       "      <th>hate</th>\n",
       "      <th>liked</th>\n",
       "      <th>movie</th>\n",
       "      <th>really</th>\n",
       "      <th>scenes</th>\n",
       "      <th>space</th>\n",
       "      <th>spaceship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.386839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.652057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576336</td>\n",
       "      <td>0.416964</td>\n",
       "      <td>0.702836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408763</td>\n",
       "      <td>0.498483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.608941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499340</td>\n",
       "      <td>0.361260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.49934</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     action     alien    aliens   awesome     awful    boring  characters  \\\n",
       "0  0.000000  0.000000  0.652057  0.000000  0.000000  0.000000    0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000   \n",
       "2  0.447214  0.000000  0.000000  0.447214  0.000000  0.447214    0.447214   \n",
       "3  0.000000  0.498483  0.000000  0.000000  0.498483  0.000000    0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000   \n",
       "\n",
       "       cool     films      hate     liked     movie    really    scenes  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.386839  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.576336  0.416964  0.702836  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.447214   \n",
       "3  0.000000  0.408763  0.498483  0.000000  0.295730  0.000000  0.000000   \n",
       "4  0.608941  0.000000  0.000000  0.499340  0.361260  0.000000  0.000000   \n",
       "\n",
       "     space  spaceship  \n",
       "0  0.00000   0.652057  \n",
       "1  0.00000   0.000000  \n",
       "2  0.00000   0.000000  \n",
       "3  0.00000   0.000000  \n",
       "4  0.49934   0.000000  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Found existing installation: en-core-web-sm 2.1.0\n",
      "    Uninstalling en-core-web-sm-2.1.0:\n",
      "      Successfully uninstalled en-core-web-sm-2.1.0\n",
      "  Running setup.py install for en-core-web-sm: started\n",
      "    Running setup.py install for en-core-web-sm: finished with status 'done'\n",
      "Successfully installed en-core-web-sm-2.1.0\n",
      "Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Linking successful\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "C:\\Users\\nuno_\\AppData\\Roaming\\Python\\Python36\\site-packages\\spacy\\data\\en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.2, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
      "You do not have sufficient privilege to perform this operation.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin 0 6 GPE\n",
      "Germany 25 32 GPE\n",
      "Angela Merkel 66 79 PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(\"\"\"Berlin is the capital of Germany; and the residence of Chancellor Angela Merkel.\"\"\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">But \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is starting from behind. The company made a late push</br>into hardware, and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Appleâ€™s Siri\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", available on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    iPhones\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "â€™s Alexa</br>software, which runs on its \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Echo and Dot\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " devices, have clear leads in\n",
       "consumer adoption.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "text = \"\"\"But Google is starting from behind. The company made a late push\n",
    "into hardware, and Appleâ€™s Siri, available on iPhones, and Amazonâ€™s Alexa\n",
    "software, which runs on its Echo and Dot devices, have clear leads in\n",
    "consumer adoption.\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(text)\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
